# -*- coding: utf-8 -*-
"""T2_ANN_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e7MuA9SClBaqqFl0ZApEGHTBnIXoF3oT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

"""# Binary Classification"""

data = pd.read_csv('/content/Iris.csv')
data = data.loc[:99,['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','Species']]
data

data.isna().sum()

data['Species'].unique()

data.describe()

data['y'] = data['Species'].map({'Iris-setosa':0,'Iris-versicolor':1})
data

X = data.drop(['Species','y'],axis=1)
y = data['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

w = np.random.randn(4,1)
b = np.random.randn(1)

w

len(w)

w[1]

w.T

X_train[0].shape

X_train[0,1]

np.dot(X_train[0],w)+b

b

num_epochs = 300

def sigmoid(x):
  return 1/(1+np.exp(-x))

def binary_loss(output,y):
  return -(y*np.log(sigmoid(output))+(1-y)*np.log(1-sigmoid(output)))

def gradient_descent_w(X,y,w,b,learning_rate=0.01):
  w_new = []
  for i in range(len(w)):
    w_new.append(w[i] - learning_rate*(sigmoid(np.dot(X,w)+b)-y)*X[i])
  return w_new

def gradient_descent_b(X,y,w,b,learning_rate=0.01):
  b_new  = b - learning_rate*(sigmoid(np.dot(X,w)+b)-y)
  return b_new

for epoch in range(num_epochs):
  for i in range(len(X_train)):
    x = X_train[i]
    y = y_train[i]
    output = sigmoid(np.dot(x,w)+b)
    loss = binary_loss(output,y)


    w_temp = gradient_descent_w(x,y,w,b)
    b_temp = gradient_descent_b(x,y,w,b)
    w = w_temp
    b = b_temp

  print(f'epoch: {epoch}, loss: {loss}')

sigmoid(np.dot(X_test[0],w)+b)

w

y_test[0]

def accuracy(X,y):
  correct = 0
  total = len(X)
  for i in range(len(X)):
    x = X[i]
    y_pred = sigmoid(np.dot(x,w)+b)
    if(y_pred > 0.5):
      y_pred = 1
    else:
      y_pred = 0
    if(y_pred == y[i]):
      correct += 1
  return (correct/total)*100

accuracy(X_test,y_test)

y_test[-1]

sigmoid(np.dot(X_test[-1],w)+b)

"""# Multi-Class Classification (MNIST)"""

import torch
import torchvision

train_dataset = torchvision.datasets.MNIST(root='./dataset',train=True,transform=torchvision.transforms.ToTensor(),download=True)
test_dataset = torchvision.datasets.MNIST(root='./dataset',train=False,transform=torchvision.transforms.ToTensor(),download=True)

train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1,shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=True)

data,label = train_dataset[0]

data

data.shape

type(data)

type(label)

label

# W = np.random.randn(784,10)
# B = np.random.randn(10)

W = W
B = B

W

B

label

def softmax(x):
  x = (np.exp(x)/sum(np.exp(x)))
  return x

np.sum(softmax(B))

def cross_entropy_loss(output,y):
  y = np.eye(10)[y]
  return -np.sum(y*np.log(output))

def gradient_descent_W(X,y,W,B,output,learning_rate=0.01):
  W_new = np.zeros(W.shape)
  y = np.eye(10)[y]
  for i in range(len(X)):
    dW = (output-y)*X[i]
    W_new[i] = W[i]-learning_rate*dW

  return W_new

def gradient_descent_B(X,y,W,B,output,learning_rate=0.01):
  B_new = np.zeros(B.shape)
  y = np.eye(10)[y]
  B_new = B - learning_rate*((output)-y)
  return B_new

for epoch in range(5):
  for i,(X,target) in enumerate(train_loader):
    X = X.numpy()
    X = X.squeeze()
    X = X.flatten()
    output = np.matmul(X,W)+B
    output = softmax(output)
    loss = cross_entropy_loss(output,target)

    W_temp = gradient_descent_W(X,target,W,B,output)
    B_temp = gradient_descent_B(X,target,W,B,output)
    W = W_temp
    B = B_temp

  print(f'epoch: {epoch}, loss: {loss}')

def check_accuracy(data_loader,W,B):
  correct = 0
  total = 0
  for i,(X,target) in enumerate(data_loader):
    X = X.numpy()
    X = X.squeeze()
    X = X.flatten()
    output = np.matmul(X,W)+B
    output = softmax(output)
    pred = np.argmax(output)
    if(pred == target):
      correct += 1
    total += 1
  return (correct/total)*100

check_accuracy(test_loader,W,B)

