# -*- coding: utf-8 -*-
"""T3_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16x1Q0RBKfSMLUciCFhL58q_ZJESzDwBw
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision

num_epochs = 10
sequence_length = 28
hidden_size = 256
input_size = 28
num_layers = 2
num_classes = 10

class RNN_MNIST(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers, num_classes):
    super(RNN_MNIST, self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
    self.fc = nn.Linear(hidden_size*sequence_length, num_classes)

  def forward(self, x):
    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
    out, _ = self.rnn(x, h0)
    out = out.reshape(out.shape[0], -1)
    out = self.fc(out)
    return out

train_dataset = torchvision.datasets.MNIST(root='./dataset',train=True,transform=torchvision.transforms.ToTensor(),download=True)
test_dataset = torchvision.datasets.MNIST(root='./dataset',train=False,transform=torchvision.transforms.ToTensor(),download=True)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=64,shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=64,shuffle=True)

model = RNN_MNIST(input_size, hidden_size, num_layers, num_classes)

optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(num_epochs):
  for i, (data, labels) in enumerate(train_loader):
    data = data.squeeze(1)

    outputs = model(data)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()

    optimizer.step()

def check_accuracy(loader, model):
  correct = 0
  total = 0
  model.eval()

  with torch.no_grad():
    for data, labels in loader:
      data = data.squeeze(1)
      outputs = model(data)
      _, predicted = outputs.max(1)
      total += predicted.size(0)
      correct += (predicted == labels).sum().item()
  return (correct / total)*100

  model.train()

check_accuracy(test_loader, model)

"""# Time series"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

num_epochs = 6
sequence_length = 50
hidden_size = 256
input_size = 1
num_layers = 2

data = pd.read_csv('/content/akshat.csv')

time_data = data['value']

plt.plot(time_data[:100])
plt.show()

time_data

"""### Normalization"""

scaled_data = MinMaxScaler().fit_transform(time_data.values.reshape(-1,1))

scaled_data

def CustomDataset(data, sequence_length):
  data = torch.FloatTensor(data)
  for i in range(len(data)-sequence_length):
    yield data[i:i+sequence_length], data[i+sequence_length]

seq_data = CustomDataset(scaled_data, sequence_length)

# Convert the generator to a list so you can index it
seq_data_list = list(seq_data)

# Now you can access elements using indexing
seq_data_list[0]

train_data = seq_data_list[:int(len(seq_data_list)*0.95)]
test_data = seq_data_list[int(len(seq_data_list)*0.95):]

train_loader = torch.utils.data.DataLoader(dataset=train_data,batch_size=128,shuffle=False)
test_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=1,shuffle=False)

class RNN(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers):
    super(RNN, self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
    self.fc = nn.Linear(hidden_size*sequence_length, 1)

  def forward(self, x):
    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
    out, _ = self.rnn(x, h0)
    out = out.reshape(out.shape[0], -1)
    out = self.fc(out)
    return out

model = RNN(input_size, hidden_size, num_layers)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

model.train()

for epoch in range(num_epochs):
  for i, (data, labels) in enumerate(train_loader):
    outputs = model(data)
    loss = criterion(outputs, labels)

    optimizer.zero_grad()
    loss.backward()

    optimizer.step()

    print(f'Loss: {loss.item():.6f}')

def accuracy(loader, model):
  model.eval()
  abs_error = 0

  with torch.no_grad():
    for data, labels in loader:
      outputs = model(data)
      abs_error += np.abs(outputs.numpy()-labels.numpy())
  return abs_error/len(loader)

accuracy(test_loader, model)

"""### Checking on an input"""

X_test_scaled, y_true_scaled = seq_data_list[-1]

X_test_scaled = X_test_scaled.unsqueeze(0)

X_test_scaled.shape

y_pred_scaled = model(X_test_scaled)
y_pred_scaled

y_true_scaled

# Fit the scaler to your original training data (replace X_train with your training data)
scaler = MinMaxScaler()
# Reshape data to be a 2D array
scaler.fit(time_data.values.reshape(-1, 1))

# Now you can use the fitted scaler for inverse transformation
y_pred = scaler.inverse_transform(y_pred_scaled.detach().numpy().reshape(-1, 1)) # Reshape y_pred_scaled to 2D
y_true = scaler.inverse_transform(y_true_scaled.detach().numpy().reshape(-1, 1)) # Reshape y_true_scaled to 2D

y_pred

y_true

